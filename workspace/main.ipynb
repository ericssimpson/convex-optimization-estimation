{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we demonstrate the generating of polyhedrons, whos feasible region is either empty or not. \n",
    "When given an appropriate cost vector the area contains a feasible solution. \n",
    "\n",
    "Additionally, we are keeping our decision variables (n) and our constraints (m) constant, since the problem of varying input dimension to our model significnatly increases the complexlixty of the project. This topic will be explored in more detail later in the further research section. This part of the project will likely be cut out and made into its own repository.\n",
    "\n",
    "We plot the polyhedron (for n = 2) and label whether it is bounded or not. \n",
    "With the cost vector, if the feasible solution is unbounded we can simply take the dual of it and have an infeasible problem.\n",
    "We will be graphing the primal problem as demonstration.\n",
    "\n",
    "The linear program will take the standard form:\n",
    "minimize: z = c'x\n",
    "subject to: Ax <= b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear_Program_0:\n",
      "MINIMIZE\n",
      "-4*x_0 + -7*x_1 + 0\n",
      "SUBJECT TO\n",
      "_C1: 0 x_0 + 9 x_1 >= 0\n",
      "\n",
      "_C2: 0 x_0 - 4 x_1 >= -3\n",
      "\n",
      "VARIABLES\n",
      "-inf <= x_0 <= 0 Continuous\n",
      "-inf <= x_1 <= 0 Continuous\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#NOTE: Here We Detail The Generation of Random Linear Programs\n",
    "#! More Work May Need To Be Done Regarding The Making Of Truly Balanced Data Through Dimensionality and Attribute Varation\n",
    "\n",
    "import pulp as pl\n",
    "import numpy as np\n",
    "\n",
    "def generate_random_linear_program(min=np.random.choice([0,1]), decision_variables=np.random.randint(-10,10), constraints=np.random.randint(-10,10), iteration=0):\n",
    "    '''\n",
    "    Creating trivally varible random linear programs.\n",
    "\n",
    "    TODO:\n",
    "    >Expand doc string\n",
    "    >Expand actual todo\n",
    "    '''\n",
    "    #? Randomly Determines Whether To Generate A Maximize or Minimize Problem\n",
    "    if min == 1:\n",
    "        linear_program = pl.LpProblem(f\"Linear_Program_{iteration}\", pl.LpMinimize)\n",
    "    else:\n",
    "        linear_program = pl.LpProblem(f\"Linear_Program_{iteration}\", pl.LpMaximize)\n",
    "\n",
    "    #? Generates a Dictionary of n Decision Variables Then Randomizes Upper or Lower Bound From Zero\n",
    "    decision_variables = pl.LpVariable.dicts(\"x\", range(decision_variables))\n",
    "    for variable in decision_variables:\n",
    "        lower = np.random.choice([0,1])\n",
    "        if lower == 1:\n",
    "            decision_variables[variable].lowBound = 0\n",
    "        else:\n",
    "            decision_variables[variable].upBound = 0\n",
    "\n",
    "    #? For Each Decision Varible Create a Random Constant Such That: \n",
    "    #? Objective Function = (Variable 1, Random Constant 1) + (Variable 2, Random Constant 2) + ... + (Variable i, Random Constant i)\n",
    "    objective_function = pl.LpAffineExpression([(decision_variables[i], np.random.randint(-10,10)) for i in range(len(decision_variables))])\n",
    "\n",
    "    #? Adds Objective Function To The Linear Program\n",
    "    linear_program += objective_function\n",
    "\n",
    "    #? Generates m Constraints With Random Constants Similar To Objective Function\n",
    "    for _ in range(constraints):\n",
    "        temporary_function = pl.LpAffineExpression([(decision_variables[i], np.random.randint(-10,10)) for i in range(len(decision_variables))])\n",
    "        equality_sign = np.random.choice([-1,1]) #? Constraint Such That >= or <= Random Constant\n",
    "        equality_value = np.random.randint(-10,10) #? Random Constraint Value\n",
    "        temporary_constraint = pl.LpConstraint(temporary_function, equality_sign, rhs=equality_value)\n",
    "        linear_program += temporary_constraint\n",
    "\n",
    "    return linear_program\n",
    "\n",
    "example_linear_program = generate_random_linear_program(decision_variables=2, constraints=2)\n",
    "print(example_linear_program)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Data Length: 10\n",
      "Example Data Balance [Feasible, Infeasible]: [5, 5]\n",
      "Example Entry: {'objective': {'name': None, 'coefficients': [{'name': 'x_0', 'value': -10}, {'name': 'x_1', 'value': 7}]}, 'constraints': [{'sense': -1, 'pi': None, 'constant': 10, 'name': None, 'coefficients': [{'name': 'x_0', 'value': 9}, {'name': 'x_1', 'value': -3}]}, {'sense': -1, 'pi': None, 'constant': -3, 'name': None, 'coefficients': [{'name': 'x_0', 'value': -6}, {'name': 'x_1', 'value': 0}]}], 'variables': [{'lowBound': None, 'upBound': 0, 'cat': 'Continuous', 'varValue': None, 'dj': None, 'name': 'x_0'}, {'lowBound': 0, 'upBound': None, 'cat': 'Continuous', 'varValue': None, 'dj': None, 'name': 'x_1'}], 'parameters': {'name': 'Linear_Program_0', 'sense': 1, 'status': 0, 'sol_status': 0}, 'sos1': [], 'sos2': []}\n"
     ]
    }
   ],
   "source": [
    "#NOTE: Here We Detial The Creation of Balanced Training Data for the Machine Learning Model\n",
    "#! More Work May Need To Be Done Regarding The Making Of Truly Balanced Data By Eliminating The Maximum Number Of Biasing Attributes\n",
    "\n",
    "import json\n",
    "'''\n",
    "#? Custom Encoder Used For NumPy Variables In Linear Program\n",
    "class NpEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        else:\n",
    "            return super(NpEncoder, self).default(obj)\n",
    "'''\n",
    "'''\n",
    "def pulp_extract(linear_program):\n",
    "\n",
    "    return a_matrix, b_vector, c_vector, b_sign, v_sign\n",
    "'''\n",
    "\n",
    "def create_balanced_data(size = 1000):\n",
    "    '''\n",
    "    Creating trivially balanced training data.\n",
    "\n",
    "    TODO:\n",
    "    >Expand doc string\n",
    "    >Expand actual todo\n",
    "    '''\n",
    "    n, m = 2, 2\n",
    "    training_data = []\n",
    "    last_category = None\n",
    "    while len(training_data) < size:\n",
    "\n",
    "        #? Creating Linear Program\n",
    "        temporary_lp = generate_random_linear_program(decision_variables=n, constraints=m)\n",
    "        solver = pl.PULP_CBC_CMD()\n",
    "\n",
    "        #* Should Check Here To Eliminate Possible Biasing Attributes\n",
    "\n",
    "        #? Formatting Linear Program\n",
    "        #lp_formatted = pulp_extract(temporary_lp)\n",
    "        objective = -1 #? 0 == Minimze, 1 == Maximize\n",
    "        a_matrix = []\n",
    "        b_vector = []\n",
    "        c_vector = []\n",
    "        b_sign = []\n",
    "        v_sign = []\n",
    "\n",
    "\n",
    "        #? Pre-Formatting Linear Program For Machine Learning Before Solving\n",
    "        '''\n",
    "        temporary_lp.to_json('preprocess_lp.json', cls=NpEncoder)\n",
    "        lp_data = open('preprocess_lp.json')\n",
    "        processed_lp = json.load(lp_data)\n",
    "        lp_data.close()\n",
    "        '''\n",
    "\n",
    "        #? Solving Linear Program For Data Label Creation\n",
    "        result = temporary_lp.solve(solver)\n",
    "        status = pl.LpStatus[temporary_lp.status]\n",
    "\n",
    "        #? Pairing Data and Label While Balancing Into Training Set\n",
    "        if status == \"Optimal\" and (last_category == None or last_category == 1):\n",
    "            lp_category = 0\n",
    "            training_data.append([lp_formatted, lp_category])\n",
    "            last_category = 0\n",
    "\n",
    "        elif status == \"Infeasible\" and (last_category == None or last_category == 0):\n",
    "            lp_category = 1\n",
    "            training_data.append([lp_formatted, lp_category])\n",
    "            last_category = 1\n",
    "\n",
    "    return training_data\n",
    "\n",
    "example_training_data = create_balanced_data(10)\n",
    "print(f\"Example Data Length: {len(example_training_data)}\")\n",
    "\n",
    "category_count = [0,0]\n",
    "for item in example_training_data:\n",
    "    if item[1] == 0:\n",
    "        category_count[0] += 1\n",
    "    else:\n",
    "        category_count[1] += 1\n",
    "print(f\"Example Data Balance [Feasible, Infeasible]: {category_count}\")\n",
    "print(f\"Example Entry: {example_training_data[0][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Length: 1000\n",
      "Training Data Balance [Feasible, Infeasible]: [500, 500]\n",
      "Testing Data Length: 1000\n",
      "Testing Data Balance [Feasible, Infeasible]: [500, 500]\n"
     ]
    }
   ],
   "source": [
    "#NOTE: Here We Generate The Training and Testing Data\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "training_data = create_balanced_data(1000)\n",
    "print(f\"Training Data Length: {len(training_data)}\")\n",
    "category_count = [0,0]\n",
    "for item in training_data:\n",
    "    if item[1] == 0:\n",
    "        category_count[0] += 1\n",
    "    else:\n",
    "        category_count[1] += 1\n",
    "print(f\"Training Data Balance [Feasible, Infeasible]: {category_count}\")\n",
    "random.shuffle(training_data)\n",
    "\n",
    "testing_data = create_balanced_data(1000)\n",
    "print(f\"Testing Data Length: {len(testing_data)}\")\n",
    "category_count = [0,0]\n",
    "for item in testing_data:\n",
    "    if item[1] == 0:\n",
    "        category_count[0] += 1\n",
    "    else:\n",
    "        category_count[1] += 1\n",
    "print(f\"Testing Data Balance [Feasible, Infeasible]: {category_count}\")\n",
    "random.shuffle(testing_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type NoneType).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Eric Simpson\\LP-Estimation\\workspace\\main.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 38>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Eric%20Simpson/LP-Estimation/workspace/main.ipynb#ch0000014?line=31'>32</a>\u001b[0m model\u001b[39m.\u001b[39madd(tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mDense(\u001b[39m2\u001b[39m, activation\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39msoftmax))  \u001b[39m# our output layer. 10 units for 10 classes. Softmax for probability distribution\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Eric%20Simpson/LP-Estimation/workspace/main.ipynb#ch0000014?line=33'>34</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m,  \u001b[39m# Good default optimizer to start with\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Eric%20Simpson/LP-Estimation/workspace/main.ipynb#ch0000014?line=34'>35</a>\u001b[0m               loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msparse_categorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m,  \u001b[39m# how will we calculate our \"error.\" Neural network aims to minimize loss.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Eric%20Simpson/LP-Estimation/workspace/main.ipynb#ch0000014?line=35'>36</a>\u001b[0m               metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])  \u001b[39m# what to track\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Eric%20Simpson/LP-Estimation/workspace/main.ipynb#ch0000014?line=37'>38</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(x_train, y_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m)  \u001b[39m# train the model\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Eric%20Simpson/LP-Estimation/workspace/main.ipynb#ch0000014?line=39'>40</a>\u001b[0m val_loss, val_acc \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate(x_test, y_test)  \u001b[39m# evaluate the out of sample data with model\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Eric%20Simpson/LP-Estimation/workspace/main.ipynb#ch0000014?line=40'>41</a>\u001b[0m \u001b[39mprint\u001b[39m(val_loss)  \u001b[39m# model's loss (error)\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Python310/lib/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Python310/lib/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> <a href='file:///c%3A/Python310/lib/site-packages/keras/utils/traceback_utils.py?line=66'>67</a>\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     <a href='file:///c%3A/Python310/lib/site-packages/keras/utils/traceback_utils.py?line=67'>68</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Python310/lib/site-packages/keras/utils/traceback_utils.py?line=68'>69</a>\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Python310/lib/site-packages/tensorflow/python/framework/constant_op.py?line=99'>100</a>\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[0;32m    <a href='file:///c%3A/Python310/lib/site-packages/tensorflow/python/framework/constant_op.py?line=100'>101</a>\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m--> <a href='file:///c%3A/Python310/lib/site-packages/tensorflow/python/framework/constant_op.py?line=101'>102</a>\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type NoneType)."
     ]
    }
   ],
   "source": [
    "#NOTE: Here We Preprocess The Training Data\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import TextVectorization\n",
    "\n",
    "import pickle\n",
    "\n",
    "x_train = pickle.load(open(\"x_train.pickle\", \"rb\"))\n",
    "x_test = pickle.load(open(\"x_test.pickle\", \"rb\"))\n",
    "\n",
    "y_train = pickle.load(open(\"y_train.pickle\", \"rb\"))\n",
    "y_train = np.array(y_train)\n",
    "y_test = pickle.load(open(\"y_test.pickle\", \"rb\"))\n",
    "y_test = np.array(y_train)\n",
    "\n",
    "vectorizer = TextVectorization(output_mode=\"int\")\n",
    "x_train = vectorizer.adapt(x_train)\n",
    "x_train = np.array(x_train)\n",
    "\n",
    "x_test = vectorizer.adapt(x_test)\n",
    "x_test = np.array(x_test)\n",
    "\n",
    "#x_train = tf.keras.utils.normalize(x_train, axis=1)  # scales data between 0 and 1\n",
    "#x_test = tf.keras.utils.normalize(x_test, axis=1)  # scales data between 0 and 1\n",
    "\n",
    "model = tf.keras.models.Sequential()  # a basic feed-forward model\n",
    "model.add(tf.keras.layers.Flatten())  # takes our 28x28 and makes it 1x784\n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))  # a simple fully-connected layer, 128 units, relu activation\n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))  # a simple fully-connected layer, 128 units, relu activation\n",
    "model.add(tf.keras.layers.Dense(2, activation=tf.nn.softmax))  # our output layer. 10 units for 10 classes. Softmax for probability distribution\n",
    "\n",
    "model.compile(optimizer='adam',  # Good default optimizer to start with\n",
    "              loss='sparse_categorical_crossentropy',  # how will we calculate our \"error.\" Neural network aims to minimize loss.\n",
    "              metrics=['accuracy'])  # what to track\n",
    "\n",
    "model.fit(x_train, y_train, epochs=3)  # train the model\n",
    "\n",
    "val_loss, val_acc = model.evaluate(x_test, y_test)  # evaluate the out of sample data with model\n",
    "print(val_loss)  # model's loss (error)\n",
    "print(val_acc)  # model's accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE Messing Around\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mnist = tf.keras.datasets.mnist  # mnist is a dataset of 28x28 images of handwritten digits and their labels\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()  # unpacks images to x_train/x_test and labels to y_train/y_test\n",
    "\n",
    "x_train = tf.keras.utils.normalize(x_train, axis=1)  # scales data between 0 and 1\n",
    "x_test = tf.keras.utils.normalize(x_test, axis=1)  # scales data between 0 and 1\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=3)  # train the model\n",
    "\n",
    "val_loss, val_acc = model.evaluate(x_test, y_test)  # evaluate the out of sample data with model\n",
    "print(val_loss)  # model's loss (error)\n",
    "print(val_acc)  # model's accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('epic_num_reader.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = tf.keras.models.load_model('epic_num_reader.model')\n",
    "predictions = new_model.predict(x_test)\n",
    "print(np.argmax(predictions[0]))\n",
    "plt.imshow(x_test[0],cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: Here Is Just Messing Around and Graphing Of Linear Programs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from scipy.spatial import HalfspaceIntersection, ConvexHull\n",
    "from scipy.optimize import linprog\n",
    "\n",
    "import pulp\n",
    "\n",
    "def feasible_point(A, b):\n",
    "    # finds the center of the largest sphere fitting in the convex hull\n",
    "    norm_vector = np.linalg.norm(A, axis=1)\n",
    "    A_ = np.hstack((A, norm_vector[:, None]))\n",
    "    b_ = b[:, None]\n",
    "    c = np.zeros((A.shape[1] + 1,))\n",
    "    c[-1] = -1\n",
    "    res = linprog(c, A_ub=A_, b_ub=b[:, None], bounds=(None, None))\n",
    "    return res.x[:-1]\n",
    "    \n",
    "def hs_intersection(A, b):\n",
    "    interior_point = feasible_point(A, b)\n",
    "    halfspaces = np.hstack((A, -b[:, None]))\n",
    "    hs = HalfspaceIntersection(halfspaces, interior_point)\n",
    "    return hs\n",
    "\n",
    "def plt_halfspace(a, b, bbox, ax):\n",
    "    if a[1] == 0:\n",
    "        ax.axvline(b / a[0])\n",
    "    else:\n",
    "        x = np.linspace(bbox[0][0], bbox[0][1], 100)\n",
    "        ax.plot(x, (b - a[0]*x) / a[1])\n",
    "\n",
    "def add_bbox(A, b, xrange, yrange):\n",
    "    A = np.vstack((A, [\n",
    "        [-1,  0],\n",
    "        [ 1,  0],\n",
    "        [ 0, -1],\n",
    "        [ 0,  1],\n",
    "    ]))\n",
    "    b = np.hstack((b, [-xrange[0], xrange[1], -yrange[0], yrange[1]]))\n",
    "    return A, b\n",
    "\n",
    "def solve_convex_set(A, b, bbox, ax=None):\n",
    "    A_, b_ = add_bbox(A, b, *bbox)\n",
    "    interior_point = feasible_point(A_, b_)\n",
    "    hs = hs_intersection(A_, b_)\n",
    "    points = hs.intersections\n",
    "    hull = ConvexHull(points)\n",
    "    return points[hull.vertices], interior_point, hs\n",
    "\n",
    "def plot_convex_set(A, b, bbox, ax=None):\n",
    "    # solve and plot just the convex set (no lines for the inequations)\n",
    "    points, interior_point, hs = solve_convex_set(A, b, bbox, ax=ax)\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_xlim(bbox[0])\n",
    "    ax.set_ylim(bbox[1])\n",
    "    ax.fill(points[:, 0], points[:, 1], 'r')\n",
    "    return points, interior_point, hs\n",
    "\n",
    "def plot_inequalities(A, b, c, bbox, ax=None):\n",
    "    # solve and plot the convex set,\n",
    "    # the inequation lines, and\n",
    "    # the interior point that was used for the halfspace intersections\n",
    "    points, interior_point, hs = plot_convex_set(A, b, bbox, ax=ax)\n",
    "    #ax.plot(*interior_point, 'o')\n",
    "    print(interior_point)\n",
    "    for a_k, b_k in zip(A, b):\n",
    "        plt_halfspace(a_k, b_k, bbox, ax)\n",
    "    return points, interior_point, hs\n",
    "\n",
    "def generate_linear_program(n, m, lower_entry_bound, upper_entry_bound):\n",
    "\n",
    "    a_matrix = np.random.randint(lower_entry_bound,upper_entry_bound,size = (m,n))\n",
    "    b_vector = np.random.randint(lower_entry_bound,upper_entry_bound,size = (m))\n",
    "    c_vector = np.random.randint(lower_entry_bound,upper_entry_bound,size = (n))\n",
    "\n",
    "    # Making Ax <= b with b being positive\n",
    "    for i in range(len(b_vector)):\n",
    "        if b_vector[i] <= 0:\n",
    "            b_vector[i] = b_vector[i] * -1\n",
    "            a_matrix[i] = a_matrix[i] * -1\n",
    "    \n",
    "    return (a_matrix, b_vector, c_vector)\n",
    "\n",
    "n = 2\n",
    "m = 3\n",
    "\n",
    "lower_entry_bound = -10\n",
    "upper_entry_bound = 10\n",
    "\n",
    "a_matrix, b_vector, c_vector = generate_linear_program(n,m,lower_entry_bound,upper_entry_bound)\n",
    "\n",
    "print(f'A \\n{a_matrix}\\n')\n",
    "print(f'b \\n{b_vector}\\n')\n",
    "print(f'c \\n{c_vector}\\n')\n",
    "\n",
    "linear_program = pulp.LpProblem(\"Test Generation\", pulp.LpMinimize)\n",
    "solver = pulp.PULP_CBC_CMD()\n",
    "#? Have to figure out a way to dynamically generate variables for LP or use a differnt method to check infeasible/unbounded\n",
    "x1 = pulp.LpVariable('x1', cat='Continuous')\n",
    "x2 = pulp.LpVariable('x2', cat='Continuous')\n",
    "linear_program += c_vector[0] * x1 + c_vector[1] * x2, 'Z'\n",
    "for i in range(len(a_matrix)):\n",
    "    linear_program += a_matrix[i][0] * x1 + a_matrix[i][1] * x2 <= b_vector[i]\n",
    "print(linear_program)\n",
    "result = linear_program.solve(solver)\n",
    "status = pulp.LpStatus[linear_program.status]\n",
    "print(status)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10, 10)\n",
    "bbox = [(-10, 10), (-10, 10)]\n",
    "fig, ax = plt.subplots()\n",
    "plot_inequalities(a_matrix, b_vector, c_vector, bbox, ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: Here is Just Testing Of The Random Linear Program Generation And JSON Outputs\n",
    "import json\n",
    "\n",
    "class NpEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        else:\n",
    "            return super(NpEncoder, self).default(obj)\n",
    "\n",
    "#? Testing Data Distribution\n",
    "#! More Work May Need To Be Done Regarding Making Truly Balanced Data With No Biasing Variables\n",
    "type_distribution = [0,0,0,0] #? Feasible, Unbounded, Infeasible, Error\n",
    "n, m = 2, 2\n",
    "for i in range(1):\n",
    "    temporary_lp = generate_random_linear_program(decision_variables=n, constraints=m)\n",
    "    temporary_lp.to_json(\"preproccessing_lp1.json\", cls=NpEncoder)\n",
    "    solver = pl.PULP_CBC_CMD()\n",
    "    result = temporary_lp.solve(solver)\n",
    "    status = pl.LpStatus[temporary_lp.status]\n",
    "    temporary_lp.to_json(\"preproccessing_lp2.json\", cls=NpEncoder)\n",
    "\n",
    "    if status == \"Optimal\":\n",
    "        type_distribution[0] += 1\n",
    "    elif status == \"Unbounded\":\n",
    "        type_distribution[1] += 1\n",
    "    elif status == \"Infeasible\":\n",
    "        type_distribution[2] += 1\n",
    "    else:\n",
    "        type_distribution[3] += 1\n",
    "\n",
    "    data1 = open('preproccessing_lp1.json')\n",
    "    meat1 = json.load(data1)\n",
    "    data1.close()\n",
    "\n",
    "    data2 = open('preproccessing_lp2.json')\n",
    "    meat2 = json.load(data2)\n",
    "    data2.close()\n",
    "\n",
    "    print(temporary_lp)\n",
    "    print(status)\n",
    "    print(meat1)\n",
    "    print(meat2)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
